{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Gaussian Processes\n",
    "\n",
    "Gaussian process regression is computationally taxing. In particular the inference step has a time complexity of $\\mathcal{O}(N^3)$. Identifying the posterior GP scales cubically with the number of trainging examples and requires us to store all of the training information in memory. This is caused by the requirment to invert the $N \\times N$ covariance matrix which renders these methods computationally intractable for large datasets.\n",
    "\n",
    "Sparse GPs have been proposed that approximate the true posterior with a set of pseudo-training points, named inducing points. The number of such inducing points is user defined enabling control over memory and time complexity. Sparse GPs do not enjoy the benifit of closed form solutions hence one has to resort to approximate inference. \n",
    "\n",
    "One such coice for sparse approximations is variational inference. Variational inference is used to find an optimal approximate posterior. Variational inference is a method for approximating distributions that uses an optimisation process over parameters to find the best approximation among a given family. This is performed by maximising a lower bound of the log marginal liklihood. This can alternatively be posed as minimising the Kullbach-Liebler divergence.\n",
    "\n",
    "This method paves the way to a framework in which pseudo-training examples are treated as optimisation arguments of an approximate posterior, jointly identified together with hyperparameters of the generative model. \n",
    "\n",
    "The model is extensible to many supervised learning problems from regreesion with heteroscedastic and non-Gaussian Liklihoods and classification problems with discrete lables to multilabel problems.\n",
    "\n",
    "This tutorial will provide an insight into the basic matter of VI. A basic familliarity of GPs is expected.\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "Sparse GPs arise from necessity to compute the posterior GP which requires one to store and invert an $(N \\times N)$-matrix where $N$ represents the number of training observations. Sparse GPs limit the amount of pseudo data used to represent the posterior. \n",
    "\n",
    "The aim is to find a spare GP that is as close as possible to the true intractable posterior GP. The measure used to define as close as possible is the Kullbach-Liebler (KL) divergence between the sparse GP and the true posterior GP. We therfore look to find the pseudo-data such that the KL divergence becomes minimal. It is not normally possible to identify such optimal sparse GPs as closed-form solutions.\n",
    "\n",
    "One means of approximating optimal sparse GPs is to use variational inference (not the only way), which is equivalent to minimising the KL divergence. Other examples include Markov-Chain Monte Carlo methods and expectation propagation. In this tutorial we will focus on VI. \n",
    "\n",
    "VI is a particular type of approximation technique that converts the problem of Bayesian inference into an optimisation objective w.r.t the paremeters of the approximate posterior. An interesting trait is that the objective is a lower bound to the logarithm of the marginal distribution, which enables joint optimisation over the hyperparameters as well as the approximate posterior parameters. VI provides a solution to arbitrary liklihoods where the approximate posterior is typically not Gaussian (think logistic regression). \n",
    "\n",
    "In this tutorial we will provide an overview of sparse GPs, approximate inference giving background on VI, the application of VI in GPs.\n",
    "\n",
    "\n",
    "## Sparse GPs\n",
    "\n",
    "GPs can be imagined as a generalisation of multivariate Gaussians that are indexed by a input domain rather than index set. Exact and approximate texhniques leverage conditioning operations that are conceptually equivalent to those in multivariate Gaussians. This means the steps to arrive at a solution in the infinite dimensional case are similar to those in the finite dimensional case. Conditioning operations introduced herin provide a natural way to express sparse GPs and generalise readilly to interdomain GPs, GPs with multiple outputs and deep GPs, consisting of multioutput GPs stacked ontop of one another.\n",
    "\n",
    "\n",
    "### Multivariate Gaussian Identities for Conditioning\n",
    "We start by noting that conditionals on multivariate Gaussians are Gaussians as well. Take a multivariate Gaussian $\\mathcal{N}$ who's random variables are partitioned into variables $\\mathbf{f}$ and $\\mathbf{u}$. The joint distribution will assume the following form. \n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{f}\\\\\n",
    "\\mathbf{u}\n",
    "\\end{pmatrix} \\approx \\mathcal{N}\\left(\\begin{pmatrix}\n",
    "\\mathbf{\\mu_f}\\\\\n",
    "\\mathbf{\\mu_u}\n",
    "\\end{pmatrix}, \\begin{pmatrix}\n",
    "\\mathbf{\\Sigma_{ff}} & \\mathbf{\\Sigma_{fu}}\\\\\n",
    "\\mathbf{\\Sigma_{uf}}& \\mathbf{\\Sigma_{uu}}\n",
    "\\end{pmatrix}\\right)\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{\\mu_f}$ and $\\mathbf{\\mu_u}$ refer to the marginal means of $\\mathbf{f}$ and $\\mathbf{u}$ respectively, and $\\mathbf{\\Sigma_{ff}}$, $\\mathbf{\\Sigma_{fu}}$, $\\mathbf{\\Sigma_{uf}}$ and $\\mathbf{\\Sigma_{uu}}$ make up the variance covariance matrix between $\\mathbf{f}$ and $\\mathbf{u}$. The conditional distribution can then be expressed as \n",
    "$$\n",
    "\\mathbf{f}|\\mathbf{u} \\approx \\mathcal{N}\\left(\\mathbf{\\mu_f} + \\mathbf{\\Sigma_{fu}\\Sigma_{uu}^{-1}(\\mathbf{u-\\mu_u})}, \\mathbf{\\Sigma_{ff}} - \\mathbf{\\Sigma_{fu}}^T \\mathbf{\\Sigma_{uu}}^{-1} \\mathbf{\\Sigma_{uf}}\\right)\n",
    "$$\n",
    "\n",
    "Imagine that rather than the marginal distribution with mean $\\mathbf{\\mu_u}$ and covariance $\\mathbf{\\sigma_{uu}}$ there is another Gaussian distribution over $\\mathbf{u}$ with mean $\\mathbf{m}_u$ and covariance $\\mathbf{S}_{uu}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{u} \\approx \\mathcal{N}(\\mathbf{m}_u, \\mathbf{S}_{uu})\n",
    "$$\n",
    "\n",
    "Denoting the distribution $\\mathbf{f}|\\mathbf{u}$ as $p(\\mathbf{f}|\\mathbf{u})$ and the additional distribution over $\\mathbf{u}$ as $q(\\mathbf{u})$. One can obtain a marginal distribution over $\\mathbf{f}$ as $q(\\mathbf{f}) = \\int p(\\mathbf{f}|\\mathbf{u})q(\\mathbf{u})du$ that is again Gaussian.\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{f} \\approx \\mathcal{N}(\\mathbf{\\mu}_f + \\mathbf{\\Sigma}_{fu}\\mathbf{\\Sigma}_{uu}^{-1}(\\mathbf{m_u}-\\mathbf{\\mu}_u), \\mathbf{\\Sigma}_{ff} - \\mathbf{\\Sigma}_{fu}\\mathbf{\\Sigma}_{uu}^{-1}(\\mathbf{\\Sigma}_{uu}-\\mathbf{S}_{uu})\\mathbf{\\Sigma}_{uu}^{-1}\\mathbf{\\Sigma}_{uf})\n",
    "$$\n",
    "\n",
    "A sanity check reveals that if we had integrated $p(\\mathbf{f}|\\mathbf{u})$ with the marginal distribution with mean $\\mathbf{\\mu_u}$ and covariance $\\mathbf{\\sigma_{uu}}$, we would have recovered the original marginal distribution $p(\\mathbf{f})$ with mean $\\mathbf{\\mu}_f$ and covariance $\\mathbf{\\Sigma}_ff$.\n",
    "\n",
    "The equations above remain valid if we define $\\mathbf{u}$ as $\\mathbf{u}=\\mathbf{\\Phi f}$ via a linear transformation $\\mathbf{\\Phi}$. Since $\\mathbf{\\mu}_f$ and $\\mathbf{\\Sigma_{ff}}$ are given, the only remaining quantities to be identified are the mean $\\mathbf{\\mu}_u$ and the covariance matrices $\\mathbf{\\Sigma}_{fu}$,  $\\mathbf{\\Sigma}_{uf}$ and  $\\mathbf{\\Sigma}_{uu}$, yeilding:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "& \\mathbf{\\mu}_u = \\mathbf{\\Phi \\mu}_f \\\\\n",
    "& \\mathbf{\\Sigma_{fu}} = \\mathbf{\\Sigma}_{ff} \\mathbf{\\Phi}^T = (\\mathbf{\\Phi}\\mathbf{\\Sigma}_{ff})^T = \\mathbf{\\Sigma}_{uf}^T \\\\\n",
    "& \\mathbf{\\Sigma}_{uu} = \\mathbf{\\Phi \\Sigma_{ff} \\Phi}^T\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The joint covariance matrix of $\\mathbf{f}$ and $\\mathbf{u}$ is degenerate (i.e singular) because $\\mathbf{u}$ is the result of a linear transformation of $\\mathbf{f}$ and hence completely determined by $\\mathbf{f}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "https://arxiv.org/abs/2012.13962 - A tutorial on sparse gaussian processes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
